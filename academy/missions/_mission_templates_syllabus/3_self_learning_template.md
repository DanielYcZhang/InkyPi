# InkyPi Academy â€” Self-Learning Module Template

Use this module inside any mission to build reading, explaining, and debugging habits.
It is designed to be auto-validated with lightweight checks.

---

## Required Files (per mission)
Create these in the mission folder:
- `explain.md`
- `debug_detective.md` (replaces `predict.md`)
- `bad_code.py`
- `bad_code_explain.md`
- `copilot_input.txt` (auto-generated by `check.py`)

---

## explain.md (Explain-Back with New Example)
Learner demonstrates understanding by explaining a **different example** using the same concepts.

**Format:**

### Part 1: New Example Code
Provide 15-25 lines of code that uses the SAME patterns from the mission but in a different context.
- For a class/object mission, use a different domain (Vehicle, Book, RPGCharacter)
- For a layout mission, use a different arrangement (vertical instead of horizontal)

### Part 2: Line-Specific Questions
Referring to the new example code:
1) **Line X**: [specific line of code]
   - What is [specific element] in this line?
   - Why do we need it?
   - What would happen if you changed [specific part]?

2) **Line Y**: [specific line of code]
   - Is this a [concept A] or [concept B]? How do you know?
   - Explain what this line does in your own words.

3) **Line Z**: [specific line of code]
   - [Question that tests deeper understanding]
   - If you modified this to [alternative], what would change?

### Part 3: Concept Transfer Challenge
Apply the concepts to something completely new:
- Design a [new class/structure] with [properties/elements]
- Explain in 2-3 sentences why using [core concept] is better than [alternative approach]

**Goal**: Test if the learner can RECOGNIZE and APPLY patterns in unfamiliar code, not just recite definitions.

---

## bad_code.py + bad_code_explain.md (Progressive Refactoring Challenge)

**Philosophy**: Don't just show bad code and say "fix it." Make the learner EXPERIENCE the pain of changing bad code, then feel the relief when using proper concepts.

### bad_code.py Structure (4-Stage Journey):

**Stage 1: Working Code (GIVEN)**
- Provides code that works TODAY but uses anti-patterns
- Important: It must actually work! Not obviously broken.

**Stage 2: Feature Request #1 (TODO using bad code)**
- Realistic change: "Add 2 more items" or "Add new property to all"
- Student modifies Stage 1 code to meet requirement
- **Track metrics**: Lines changed, places updated, pain level

**Stage 3: Refactored Version (TODO using core concept)**
- Rewrite using the mission's core concept (class, loop, function, etc.)
- Much cleaner, more maintainable

**Stage 4: Same Feature Request (TODO using good code)**
- Apply the SAME change from Stage 2
- **Compare metrics**: Dramatically fewer changes
- Experience the "aha!" moment

### bad_code_explain.md Structure (Metrics + Team Thinking):

**Part 1: Change Impact Metrics**
- How many lines changed? (Stage 2 vs Stage 4)
- How many places updated?
- What's the improvement ratio?

**Part 2: Team Collaboration Test**
- "Your teammate needs to add another item..."
- What do they need to figure out? (bad code)
- What exact line would they add? (good code)

**Part 3: Second Feature Request (Stress Test)**
- Predict impact of a DIFFERENT change
- When does refactored approach become essential?

**Part 4: When "Bad" is Acceptable**
- List specific scenarios where simple approach is OK
- Rule of thumb: 1-2 instances = OK, 3+ = refactor

**Part 5: Real-World Connection**
- Personal experiences with hard-to-change code
- Long-term maintenance thinking

**Key Improvement**: Instead of asking "why is your version better?" (abstract), ask "how many lines did you change?" (concrete and measurable).

**Example Feature Requests by Concept:**
- **Variables/Loops**: "Add 2 more boxes" - bad code needs 6 new lines, good code changes one variable
- **Classes**: "Add health property to all creatures" - bad code touches 15 lines, good code touches 2
- **Functions**: "Change text color everywhere" - bad code finds all 12 places, good code updates one function

See `academy/BAD_CODE_ENHANCEMENT.md` for complete templates and examples.

---

## debug_detective.md (Debug Challenge)
Learner practices reading error messages and debugging code.

**Format:**

### The Broken Code
Provide code with 2-3 deliberate bugs related to the mission's core concepts.

```python
[Code snippet with intentional errors]
```

### Your Mission
1. **Predict**: Before running, read the code. What do you think will happen?
   
2. **Run**: Execute the code. What error appears? (Copy the exact error message)

3. **Locate**: Which line is causing the error? How did you find it?

4. **Diagnose**: WHY is it broken? Connect the error to a concept from the briefing.
   - Example: "This fails because [concept] requires [rule], but the code [what it did wrong]"

5. **Fix**: Correct the code and verify it works.

6. **Explain**: What did this bug teach you about [core concept]?

**Why this works:**
- Teaches resilience (errors are learning opportunities)
- Connects error messages to concepts
- Builds debugging mental models
- More engaging than arithmetic tracing

---

## copilot_input.txt (Copilot As Grader)
This file is generated by `check.py` and contains a ready-to-paste prompt.

```
You are a strict grader. Do not give the correct answer.
Evaluate my explanation using this rubric:
- Mentions function name and purpose
- Mentions parameters and their meaning
- Describes what changes on the screen
- Uses plain language suitable for a 12-year-old
Score each item 0-2 and explain what is missing.

Here is my explanation:
[PASTE EXPLANATION HERE]
```

**Copilot follow-up prompts (coach mode)**
- "Ask me to explain this code using a kitchen analogy."
- "Show a bad example and ask me to refactor it."
- "Ask me when I should and should not use this concept."

---

## Practice Tasks Live in the .py Template
All "Practice" and "Do-It" tasks should be written as TODOs inside the mission's `.py` template.
This keeps learning in the same place as the code.

---

## check.py - Enhanced Validation Aligned with New Philosophy

The mission check script validates completion and provides encouraging feedback.

### Philosophy

**Old approach**: Check files exist, minimum length
**New approach**: Validate structure, check for depth, celebrate skills proven

### What to Check

#### 1. Plugin Files (Basic Setup)
```python
# Standard checks (keep these)
- Plugin folder exists: src/plugins/{mission_id}/
- plugin-info.json exists and valid JSON
- Plugin ID matches folder name
- Python file exists: {mission_id}.py
- Icon exists: icon.png
```

#### 2. Self-Learning Files (Enhanced Validation)

**explain.md** - Check for structure, not just length:
```python
# Minimum 1000 characters (new examples are detailed)
# Keywords to check:
# - Different example name (e.g., "vehicle" for C02, not "creature")
# - Line-specific markers: "line 13", "line 18"
# - Transfer challenge keywords: mission-specific (e.g., "weapon", "sword")

if _check_text_file(explain_path, 1000, keywords=[
    "vehicle", "electriccar",  # Different example (C02: not creature)
    "line", "line 13",         # Line-specific questions
    "weapon", "sword"          # Transfer challenge
]):
    print_result("explain.md: New example with line-specific analysis âœ“", True)
```

**debug_detective.md** - Replaces predict.md and trace.md:
```python
# Minimum 300 characters (detailed debugging analysis)
# Keywords: "bug", "error", "fix", "diagnose"

if _check_text_file(debug_detective_path, 300, keywords=[
    "bug", "error", "fix", "diagnose"
]):
    print_result("debug_detective.md: Debugging analysis completed âœ“", True)
```

**bad_code.py** - Check for 4-stage structure:
```python
# Read full content to check structure
bad_code_text = bad_code_py.read_text(encoding="utf-8")

# Check for all 4 stages
has_all_stages = all([
    "Stage 1" in bad_code_text or "STAGE 1" in bad_code_text,
    "Stage 2" in bad_code_text or "STAGE 2" in bad_code_text,
    "Stage 3" in bad_code_text or "STAGE 3" in bad_code_text,
    "Stage 4" in bad_code_text or "STAGE 4" in bad_code_text
])

if has_all_stages and len(bad_code_text) >= 500:
    print_result("bad_code.py: All 4 stages present âœ“", True)
else:
    print_result("bad_code.py: Missing stages or incomplete", False)
```

**bad_code_explain.md** - Check for 5-part structure with metrics:
```python
# Minimum 1500 characters (33 questions is detailed!)
# Keywords for each part:
# Part 1: "lines changed", "improvement ratio"
# Part 2: "teammate", "collaboration"
# Part 3: "feature request", "scalability"
# Part 4: "rule of three", "when"
# Part 5: "real-world"

if _check_text_file(bad_code_explain_path, 1500, keywords=[
    "lines changed",      # Part 1: Metrics
    "teammate",          # Part 2: Team
    "improvement ratio", # Part 3: Scalability
    "rule of three",     # Part 4: When not to
    "real-world"         # Part 5: Connection
]):
    print_result("bad_code_explain.md: 5-part analysis with metrics âœ“", True)
```

#### 3. Copilot Input Generation (Updated Rubric)

```python
def _write_copilot_input(path: Path, explain_text: str, mission_concept: str) -> None:
    """
    Generate copilot_input.txt with mission-specific rubric
    
    Args:
        path: Where to write copilot_input.txt
        explain_text: Content of explain.md
        mission_concept: e.g., "inheritance", "classes", "loops"
    """
    rubric = [
        f"- Part 1: Answered questions about different example (not the mission code)",
        f"- Part 2: Explained line-specific code with understanding",
        f"- Part 3: Comparison questions show deep thinking",
        f"- Part 4: Transfer challenge completed (new system designed)",
        f"- Part 5: Connected patterns to mission code",
        f"- Shows understanding of {mission_concept}, not just memorization",
        f"- Uses plain language but demonstrates depth"
    ]
    
    prompt = [
        "You are a strict grader helping a student learn programming.",
        "Do not give the correct answer. Instead, ask probing questions.",
        "",
        "Evaluate this explanation using the rubric:",
        *rubric,
        "",
        "For each item:",
        "- Score 0 (missing), 1 (partial), or 2 (complete)",
        "- If < 2, ask a question to guide deeper thinking",
        "",
        "Here is the student's explanation:",
        "=" * 60,
        explain_text.strip(),
        "=" * 60,
    ]
    path.write_text("\n".join(prompt), encoding="utf-8")
```

### Enhanced Feedback

After validation, provide skills-based feedback:

```python
def print_completion_feedback(mission_name: str, core_concepts: list):
    """Print encouraging feedback highlighting skills proven"""
    print(f"\n{GREEN}{'='*60}{RESET}")
    print(f"{GREEN}MISSION COMPLETE: {mission_name}{RESET}")
    print(f"{GREEN}{'='*60}{RESET}\n")
    
    print("ðŸŽ¯ Skills You've Proven:\n")
    for concept in core_concepts:
        print(f"  âœ… {concept}")
    
    print("\nðŸ“Š What This Means:")
    print("  â€¢ Not just 'it works' but 'I understand WHY'")
    print("  â€¢ Ready to apply these patterns in new situations")
    print("  â€¢ Can explain concepts with concrete examples\n")
    
    print("ðŸ”„ Next Steps:")
    print("  1. Review your metrics from bad_code_explain.md")
    print("  2. Can you recall the improvement ratio?")
    print("  3. Try applying this pattern to a different domain\n")

# Example usage:
if all_passed:
    print_completion_feedback(
        "Inheritance Practice",
        [
            "Inheritance prevents code duplication (6:1 improvement)",
            "super() reuses parent methods",
            "Pattern recognition (Vehicle â†’ Weapon systems)",
            "Knowing when NOT to use inheritance (Rule of Three)"
        ]
    )
```

### Error Guidance

When checks fail, guide student to what's missing:

```python
def print_helpful_error(file_name: str, issue: str):
    """Provide actionable guidance for failed checks"""
    print_result(f"{file_name}: {issue}", False)
    
    hints = {
        "explain.md": [
            "â†’ Remember: Use a DIFFERENT example (not the mission code)",
            "â†’ Answer line-specific questions (Line 13: ...)",
            "â†’ Complete the transfer challenge (design new system)"
        ],
        "debug_detective.md": [
            "â†’ Find all bugs in the broken code",
            "â†’ Explain WHY each is broken (connect to concepts)",
            "â†’ Show how you fixed them"
        ],
        "bad_code_explain.md": [
            "â†’ Part 1: Count lines changed (Stage 2 vs Stage 4)",
            "â†’ Part 2: Answer 'teammate adds...' questions",
            "â†’ Part 3-5: Complete all sections with specific answers"
        ]
    }
    
    if file_name in hints:
        for hint in hints[file_name]:
            print(f"  {hint}")
```

### Complete check.py Template

See `academy/missions/c03_inheritance_practice/check.py` for a complete example implementing:
- âœ… Updated file checks (debug_detective.md, not predict/trace)
- âœ… Structure validation (4 stages, 5 parts)
- âœ… Keyword checking for depth
- âœ… Skills-based completion feedback
- âœ… Helpful error guidance

### Validation Philosophy

**Don't check**:
- âŒ Correctness of answers (that's Copilot's job)
- âŒ Exact wording
- âŒ Specific numerical values

**Do check**:
- âœ… Structure is present (4 stages, 5 parts)
- âœ… Sufficient depth (character counts)
- âœ… Key concepts mentioned (keywords)
- âœ… Student made genuine effort

**Goal**: Ensure student engaged with exercises, not just skipped them. Copilot grades understanding depth.

---

## Copilot Workflow (Run â†’ Open â†’ Paste â†’ Continue)
1) Run `python3 check.py` to generate `copilot_input.txt`.
2) Open `copilot_input.txt` and copy all.
3) Start a **new Copilot Chat** titled with the mission (e.g., `C-01 Explain-Back`).
4) Paste the content and send.
5) If Copilot says you missed something, keep chatting until you fix your answer.
6) Update `explain.md` and re-run `python3 check.py` to re-validate.
